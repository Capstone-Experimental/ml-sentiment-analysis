{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Data Preprocessing"
      ],
      "metadata": {
        "id": "g87zZG63z9Vz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import tensorflow_datasets as tfds\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import os\n",
        "from google.colab import drive"
      ],
      "metadata": {
        "id": "XhAP-MYb0Aag"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QErHf4jnBAww",
        "outputId": "057b5aa9-e9e8-48dc-be48-f6f165a3a28b"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ls /content/drive/MyDrive/knowze"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UcZSPSo9znby",
        "outputId": "2b355fe9-312d-4e77-ad41-24fea5ac3cde"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "sentiment  web_scraping\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.chdir('/content/drive/MyDrive/knowze/sentiment')"
      ],
      "metadata": {
        "id": "-jCgkhcH2-QZ"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initiate Var\n",
        "vocab_size = 20000\n",
        "embedding_dim = 50\n",
        "max_length= 20"
      ],
      "metadata": {
        "id": "MFzeAiryOcFI"
      },
      "execution_count": 57,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load csv file\n",
        "\n",
        "dataset = pd.read_csv('data_sentimenv2.csv')\n",
        "\n",
        "# Cleaning Data\n",
        "dataset['text'] = dataset['text'].str.lower()  # Ubah ke lowercase\n",
        "dataset['text'] = dataset['text'].str.replace('[^\\w\\s]', '')  # Hapus karakter khusus\n",
        "\n",
        "# Turn Text into list\n",
        "sentences = dataset['text'].tolist()\n",
        "labels = dataset['sentiment'].tolist()\n",
        "\n",
        "for item in range(0,5):\n",
        "  label = labels[item]\n",
        "  print(sentences[item] + ' Labels : ')\n",
        "  print(label)\n",
        "# print(len(sentences))\n",
        "# print(len(labels))\n",
        "print(tf.shape(sentences))\n",
        "print(tf.size(labels).numpy())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2JTvDqDNVoMd",
        "outputId": "4f49571e-07c6-431a-87c5-8860e9ab3fa9"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "kursus ini sangat membantu dan informatif materi disajikan dengan baik Labels : \n",
            "1\n",
            "materi kurang jelas dan sulit dipahami saya kecewa dengan kursus ini Labels : \n",
            "0\n",
            "instruktur sangat berpengetahuan dan responsif terhadap pertanyaan peserta Labels : \n",
            "1\n",
            "saya merasa rugi telah mengikuti kursus ini tidak sesuai ekspektasi saya Labels : \n",
            "0\n",
            "kursus ini memberikan wawasan yang mendalam tentang topik yang dibahas Labels : \n",
            "1\n",
            "tf.Tensor([590], shape=(1,), dtype=int32)\n",
            "590\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-13-3f8789c8b8c1>:7: FutureWarning: The default value of regex will change from True to False in a future version.\n",
            "  dataset['text'] = dataset['text'].str.replace('[^\\w\\s]', '')  # Hapus karakter khusus\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "max_data_length = tf.size(labels).numpy()\n",
        "max_data_length"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xcY95IFOxS6e",
        "outputId": "f6e1fd67-71a1-4cf5-e660-20b8208079de"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "590"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Turn text into tokenizer words\n",
        "tokenizer = tf.keras.preprocessing.text.Tokenizer(oov_token='OOV')\n",
        "tokenizer.fit_on_texts(sentences)\n",
        "word_index = tokenizer.word_index\n",
        "# print(word_index)\n",
        "# Padding Tokenizer\n",
        "def seq_padding(sentences):\n",
        "  sequences = tokenizer.texts_to_sequences(sentences)\n",
        "  padded_seq = tf.keras.preprocessing.sequence.pad_sequences(sequences,maxlen=max_length,padding='post',truncating='post')\n",
        "  return padded_seq\n",
        "train_dataset = seq_padding(sentences)\n",
        "train_dataset = tf.constant(train_dataset)\n",
        "train_dataset"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gYSFQmElBuO3",
        "outputId": "a3f3ff71-3c68-43ac-8fbf-8bdd758c2605"
      },
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(590, 20), dtype=int32, numpy=\n",
              "array([[  6,   9,   3, ...,   0,   0,   0],\n",
              "       [ 11,   5,  29, ...,   0,   0,   0],\n",
              "       [ 22,   3, 258, ...,   0,   0,   0],\n",
              "       ...,\n",
              "       [  7,  49, 257, ...,   0,   0,   0],\n",
              "       [  7,  49, 257, ...,   0,   0,   0],\n",
              "       [  7,  49, 689, ...,   0,   0,   0]], dtype=int32)>"
            ]
          },
          "metadata": {},
          "execution_count": 48
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Persiapan label agar sesuai dengan panjang teks setelah padding\n",
        "padded_labels = []\n",
        "for label in labels:\n",
        "    # padded_label = label[:max_length]  # Ambil sebagian label sesuai dengan panjang teks yang sudah dipad\n",
        "    padded_labels.append(label)\n",
        "\n",
        "padded_labels = tf.constant(padded_labels)\n",
        "\n",
        "# Tampilkan hasil preprocessing\n",
        "print(padded_labels)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7ckfTcgGyUZq",
        "outputId": "ac8278dd-bcd5-42a2-f661-ceb1944150bd"
      },
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tf.Tensor(\n",
            "[1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 1 1 0 0 1 0 0 1\n",
            " 1 1 0 0 1 1 0 0 1 1 0 0 1 0 1 1 0 0 1 1 0 1 0 1 0 1 0 1 0 1 1 0 1 1 0 1 0\n",
            " 1 0 1 1 0 0 1 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 1 0 1 0 1 0 1 0 1 1 0\n",
            " 0 1 0 1 1 0 1 0 0 1 0 1 1 0 1 0 0 1 0 1 0 0 1 0 1 0 1 0 0 0 0 1 1 0 0 1 1\n",
            " 0 0 0 1 0 1 0 1 0 0 0 0 1 1 1 0 1 1 0 0 0 1 0 1 1 0 0 1 0 0 0 0 0 1 1 0 0\n",
            " 1 0 0 0 0 0 0 0 1 0 0 0 0 1 0 0 1 1 0 0 1 0 0 0 0 0 0 0 1 0 0 0 0 1 0 0 0\n",
            " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            " 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
            " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
            " 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1\n",
            " 1 1 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0\n",
            " 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            " 0 0 0 0 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 0 0 0 0\n",
            " 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 0 0\n",
            " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1], shape=(590,), dtype=int32)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Shuffle Data\n",
        "combined_data = tf.data.Dataset.from_tensor_slices((train_dataset, padded_labels))\n",
        "# print(list(combined_data.as_numpy_iterator())[0])\n",
        "shuffled_data = combined_data.shuffle(buffer_size=len(train_dataset))\n",
        "\n",
        "# Define training and validation sets\n",
        "training_set = shuffled_data.take(500)\n",
        "validation_set = shuffled_data.skip(500)\n",
        "\n",
        "# Configure the batch size and prefetch for better performance\n",
        "batch_size = 32\n",
        "training_set = training_set.batch(batch_size).prefetch(buffer_size=tf.data.AUTOTUNE)\n",
        "validation_set = validation_set.batch(batch_size).prefetch(buffer_size=tf.data.AUTOTUNE)"
      ],
      "metadata": {
        "id": "_K0LPduu5ErM"
      },
      "execution_count": 83,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Split Dataset [Naif]\n",
        "# training_set = train_dataset[:500]\n",
        "# validation_set = train_dataset[500:]\n",
        "\n",
        "# label_training = padded_labels[:500]\n",
        "# label_validation = padded_labels[500:]\n",
        "\n",
        "# print(training_set.shape)\n",
        "# print(label_training.shape)\n",
        "\n",
        "# print(validation_set.shape)\n",
        "# print(label_validation.shape)"
      ],
      "metadata": {
        "id": "PfKzfUvo1CtE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e825b98a-7f1a-40d7-9119-e7792399c008"
      },
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(500, 20)\n",
            "(500,)\n",
            "(90, 20)\n",
            "(90,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Create Model"
      ],
      "metadata": {
        "id": "MTmiyGK80VSX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = tf.keras.Sequential([\n",
        "  tf.keras.layers.Embedding(vocab_size, embedding_dim, input_length=max_length),\n",
        "  tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(140, return_sequences=True)),\n",
        "  tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(45, return_sequences=True)),\n",
        "  tf.keras.layers.GlobalAveragePooling1D(),\n",
        "  tf.keras.layers.Dense(128, activation='relu'),\n",
        "  tf.keras.layers.Dense(64, activation='relu'),\n",
        "  tf.keras.layers.Dense(1, activation='sigmoid'),\n",
        "])\n",
        "model.compile(\n",
        "    loss='binary_crossentropy',\n",
        "    optimizer=tf.keras.optimizers.Adam(learning_rate=0.001),\n",
        "    metrics=['accuracy']\n",
        ")\n",
        "model.fit(training_set, epochs=30, validation_data=validation_set)"
      ],
      "metadata": {
        "id": "NhsQmQDn0Uh4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7e3174d9-e1fd-4c59-9992-708c0cf27d20"
      },
      "execution_count": 84,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/30\n",
            "16/16 [==============================] - 23s 326ms/step - loss: 0.6885 - accuracy: 0.5220 - val_loss: 0.7129 - val_accuracy: 0.4556\n",
            "Epoch 2/30\n",
            "16/16 [==============================] - 2s 112ms/step - loss: 0.6063 - accuracy: 0.6320 - val_loss: 0.3135 - val_accuracy: 0.9111\n",
            "Epoch 3/30\n",
            "16/16 [==============================] - 2s 113ms/step - loss: 0.1917 - accuracy: 0.9400 - val_loss: 0.0794 - val_accuracy: 0.9778\n",
            "Epoch 4/30\n",
            "16/16 [==============================] - 2s 114ms/step - loss: 0.0819 - accuracy: 0.9740 - val_loss: 0.0703 - val_accuracy: 0.9778\n",
            "Epoch 5/30\n",
            "16/16 [==============================] - 3s 178ms/step - loss: 0.0403 - accuracy: 0.9900 - val_loss: 0.0038 - val_accuracy: 1.0000\n",
            "Epoch 6/30\n",
            "16/16 [==============================] - 2s 140ms/step - loss: 0.0237 - accuracy: 0.9940 - val_loss: 0.0044 - val_accuracy: 1.0000\n",
            "Epoch 7/30\n",
            "16/16 [==============================] - 2s 108ms/step - loss: 0.0023 - accuracy: 1.0000 - val_loss: 1.7334e-04 - val_accuracy: 1.0000\n",
            "Epoch 8/30\n",
            "16/16 [==============================] - 2s 113ms/step - loss: 8.9780e-04 - accuracy: 1.0000 - val_loss: 1.5780e-04 - val_accuracy: 1.0000\n",
            "Epoch 9/30\n",
            "16/16 [==============================] - 2s 110ms/step - loss: 0.0011 - accuracy: 1.0000 - val_loss: 5.9243e-05 - val_accuracy: 1.0000\n",
            "Epoch 10/30\n",
            "16/16 [==============================] - 2s 115ms/step - loss: 2.0942e-04 - accuracy: 1.0000 - val_loss: 7.7186e-05 - val_accuracy: 1.0000\n",
            "Epoch 11/30\n",
            "16/16 [==============================] - 4s 271ms/step - loss: 2.0043e-04 - accuracy: 1.0000 - val_loss: 2.3989e-04 - val_accuracy: 1.0000\n",
            "Epoch 12/30\n",
            "16/16 [==============================] - 3s 210ms/step - loss: 9.2681e-05 - accuracy: 1.0000 - val_loss: 7.6827e-05 - val_accuracy: 1.0000\n",
            "Epoch 13/30\n",
            "16/16 [==============================] - 2s 117ms/step - loss: 7.8023e-05 - accuracy: 1.0000 - val_loss: 1.1469e-04 - val_accuracy: 1.0000\n",
            "Epoch 14/30\n",
            "16/16 [==============================] - 2s 116ms/step - loss: 6.3360e-05 - accuracy: 1.0000 - val_loss: 5.4628e-05 - val_accuracy: 1.0000\n",
            "Epoch 15/30\n",
            "16/16 [==============================] - 2s 109ms/step - loss: 5.4875e-05 - accuracy: 1.0000 - val_loss: 3.1391e-05 - val_accuracy: 1.0000\n",
            "Epoch 16/30\n",
            "16/16 [==============================] - 2s 115ms/step - loss: 4.8146e-05 - accuracy: 1.0000 - val_loss: 4.3118e-05 - val_accuracy: 1.0000\n",
            "Epoch 17/30\n",
            "16/16 [==============================] - 3s 207ms/step - loss: 4.1578e-05 - accuracy: 1.0000 - val_loss: 8.0763e-05 - val_accuracy: 1.0000\n",
            "Epoch 18/30\n",
            "16/16 [==============================] - 2s 109ms/step - loss: 3.9343e-05 - accuracy: 1.0000 - val_loss: 3.6205e-05 - val_accuracy: 1.0000\n",
            "Epoch 19/30\n",
            "16/16 [==============================] - 2s 113ms/step - loss: 3.1040e-05 - accuracy: 1.0000 - val_loss: 4.0638e-05 - val_accuracy: 1.0000\n",
            "Epoch 20/30\n",
            "16/16 [==============================] - 2s 110ms/step - loss: 2.9303e-05 - accuracy: 1.0000 - val_loss: 2.1338e-05 - val_accuracy: 1.0000\n",
            "Epoch 21/30\n",
            "16/16 [==============================] - 2s 111ms/step - loss: 2.6093e-05 - accuracy: 1.0000 - val_loss: 2.0978e-05 - val_accuracy: 1.0000\n",
            "Epoch 22/30\n",
            "16/16 [==============================] - 2s 124ms/step - loss: 2.5848e-05 - accuracy: 1.0000 - val_loss: 3.6148e-05 - val_accuracy: 1.0000\n",
            "Epoch 23/30\n",
            "16/16 [==============================] - 4s 227ms/step - loss: 2.3013e-05 - accuracy: 1.0000 - val_loss: 2.8694e-05 - val_accuracy: 1.0000\n",
            "Epoch 24/30\n",
            "16/16 [==============================] - 2s 129ms/step - loss: 2.0083e-05 - accuracy: 1.0000 - val_loss: 2.1085e-05 - val_accuracy: 1.0000\n",
            "Epoch 25/30\n",
            "16/16 [==============================] - 2s 116ms/step - loss: 2.0906e-05 - accuracy: 1.0000 - val_loss: 1.3496e-05 - val_accuracy: 1.0000\n",
            "Epoch 26/30\n",
            "16/16 [==============================] - 2s 111ms/step - loss: 1.8817e-05 - accuracy: 1.0000 - val_loss: 2.0567e-05 - val_accuracy: 1.0000\n",
            "Epoch 27/30\n",
            "16/16 [==============================] - 2s 112ms/step - loss: 1.8274e-05 - accuracy: 1.0000 - val_loss: 1.6182e-05 - val_accuracy: 1.0000\n",
            "Epoch 28/30\n",
            "16/16 [==============================] - 2s 126ms/step - loss: 1.6700e-05 - accuracy: 1.0000 - val_loss: 1.5674e-05 - val_accuracy: 1.0000\n",
            "Epoch 29/30\n",
            "16/16 [==============================] - 6s 343ms/step - loss: 1.4799e-05 - accuracy: 1.0000 - val_loss: 1.4215e-05 - val_accuracy: 1.0000\n",
            "Epoch 30/30\n",
            "16/16 [==============================] - 3s 183ms/step - loss: 1.4640e-05 - accuracy: 1.0000 - val_loss: 1.4407e-05 - val_accuracy: 1.0000\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.History at 0x7d1a931f4070>"
            ]
          },
          "metadata": {},
          "execution_count": 84
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "******"
      ],
      "metadata": {
        "id": "nNr6tDKD0cTs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Testing Models"
      ],
      "metadata": {
        "id": "6n0Fi7Ok0ySS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Testing for sigmoid\n",
        "# Sample new words for testing\n",
        "new_words = [\"Cara membunuh orang\"]\n",
        "\n",
        "# Preprocess the new words (assuming 'tokenizer' and 'max_length' are from the training phase)\n",
        "new_sequences = tokenizer.texts_to_sequences(new_words)\n",
        "new_padded =  tf.keras.preprocessing.sequence.pad_sequences(new_sequences, maxlen=max_length, padding='post', truncating='post')\n",
        "\n",
        "# Make predictions\n",
        "predictions = model.predict(new_padded)\n",
        "\n",
        "get_predict = predictions[0]\n",
        "print(get_predict)\n",
        "if get_predict > 0.5:\n",
        "  print('positif')\n",
        "else:\n",
        "  print('negatif')\n",
        "# Prediksi"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cJD3gk6m3kmg",
        "outputId": "86f6250d-c08a-498c-d7dc-1a7aa0081cad"
      },
      "execution_count": 89,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 0s 33ms/step\n",
            "[7.9049045e-05]\n",
            "negatif\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Sample new words for testing\n",
        "new_words = [\"Cara menjadi orang pintar\"]\n",
        "\n",
        "# Preprocess the new words (assuming 'tokenizer' and 'max_length' are from the training phase)\n",
        "new_sequences = tokenizer.texts_to_sequences(new_words)\n",
        "new_padded =  tf.keras.preprocessing.sequence.pad_sequences(new_sequences, maxlen=max_length, padding='post', truncating='post')\n",
        "\n",
        "# Make predictions\n",
        "predictions = model.predict(new_padded)\n",
        "\n",
        "# Get class predictions (assuming your classes are labeled as 0, 1, 2)\n",
        "class_predictions = [tf.argmax(pred).numpy() for pred in predictions]\n",
        "print(class_predictions)\n",
        "# Prediksi\n",
        "labels_word = ['Positif', 'Negatif', 'Neutral']\n",
        "for i, word in enumerate(new_words):\n",
        "    print(f\"Word: {word}\")\n",
        "    print(f\"Predicted Class: {labels_word[class_predictions[i]]}\")\n",
        "    print()"
      ],
      "metadata": {
        "id": "_GTdV1dp02cZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "336ffdde-0ba9-4c14-b219-cf657e8493f5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 0s 51ms/step\n",
            "[2]\n",
            "Word: Cara menjadi orang pintar\n",
            "Predicted Class: Neutral\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "0PAQ9_mT5tSi"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}