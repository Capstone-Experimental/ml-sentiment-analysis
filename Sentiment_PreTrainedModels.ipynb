{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "TPU",
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "JBIRo8Xtl_2h",
        "PbbsJKch1lL5"
      ]
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/https-deeplearning-ai/tensorflow-2-public/blob/adding_C4/C4/W2/ungraded_labs/C4_W2_Lab_2_text_classification.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zX4Kg8DUTKWO"
      },
      "source": [
        "#@title Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "# https://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-N3Pz2T9mFZ-"
      },
      "source": [
        "##### Copyright 2019 The TensorFlow Authors."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "uJ5mSLQQ-4im"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "form",
        "id": "5p90mFICmFK2"
      },
      "source": [
        "#@title Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "# https://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l3JBInnkl_2a"
      },
      "source": [
        "# Setup"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "02pc7RECl_2b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e7b1a9f4-6d67-4715-b896-648676507005"
      },
      "source": [
        "try:\n",
        "    %tensorflow_version 2.x\n",
        "except:\n",
        "    pass"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Colab only includes TensorFlow 2.x; %tensorflow_version has no effect.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-B-0zxxtwFQV",
        "scrolled": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a1cc6f90-0cbb-4ee1-ee3f-4caf02c23b03"
      },
      "source": [
        "import tensorflow as tf\n",
        "import tensorflow_datasets as tfds\n",
        "import tensorflow_hub as hub\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from google.colab import drive\n",
        "tfds.disable_progress_bar()\n",
        "\n",
        "print(\"\\u2022 Using TensorFlow Version:\", tf.__version__)"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "• Using TensorFlow Version: 2.12.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JBIRo8Xtl_2h"
      },
      "source": [
        "## Download the Dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gfnt5ibsl_2i",
        "scrolled": false,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "98264b78-c55a-4dde-a939-b69aed2a82d8"
      },
      "source": [
        "splits = ['train[:60%]', 'train[-40%:]', 'test']\n",
        "\n",
        "splits, info = tfds.load(name='imdb_reviews', split=splits, as_supervised=True, with_info=True)\n",
        "\n",
        "train_data, validation_data, test_data = splits"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading and preparing dataset 80.23 MiB (download: 80.23 MiB, generated: Unknown size, total: 80.23 MiB) to /root/tensorflow_datasets/imdb_reviews/plain_text/1.0.0...\n",
            "Dataset imdb_reviews downloaded and prepared to /root/tensorflow_datasets/imdb_reviews/plain_text/1.0.0. Subsequent calls will reuse this data.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PbbsJKch1lL5"
      },
      "source": [
        "## Explore the Data [IMDB DATASET]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SIyvvYFF2DXz"
      },
      "source": [
        "The labels are either 0 or 1, where 0 is a negative review, and 1 is a positive review. We will create a list with the corresponding class names, so that we can map labels to class names later on."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3gh4Taekl_2k",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "89df8eaa-ea2f-4315-8c00-f665c8e5a912"
      },
      "source": [
        "num_train_examples = info.splits['train'].num_examples\n",
        "num_test_examples = info.splits['test'].num_examples\n",
        "num_classes = info.features['label'].num_classes\n",
        "\n",
        "print('The Dataset has a total of:')\n",
        "print('\\u2022 {:,} classes'.format(num_classes))\n",
        "\n",
        "print('\\u2022 {:,} movie reviews for training'.format(num_train_examples))\n",
        "print('\\u2022 {:,} movie reviews for testing'.format(num_test_examples))"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The Dataset has a total of:\n",
            "• 2 classes\n",
            "• 25,000 movie reviews for training\n",
            "• 25,000 movie reviews for testing\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(train_data)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cVPH01-aCUYg",
        "outputId": "842098ca-4c39-44a9-c2cc-0a7bb55a7aaf"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<_PrefetchDataset element_spec=(TensorSpec(shape=(), dtype=tf.string, name=None), TensorSpec(shape=(), dtype=tf.int64, name=None))>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1SAGQKTdl_2n"
      },
      "source": [
        "class_names = ['negative', 'positive']"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wrLs9vQr16JH"
      },
      "source": [
        "Each example consists of a sentence representing the movie review and a corresponding label. The sentence is not preprocessed in any way. Let's take a look at the first example of the training set.  "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u6lqHTTzl_2q",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "28cee57a-2097-4c6f-c690-3794174521cf"
      },
      "source": [
        "for review, label in train_data.take(1):\n",
        "    review = review.numpy()\n",
        "    label = label.numpy()\n",
        "    print('\\nMovie Review:\\n\\n', review)\n",
        "    print('\\nLabel:', class_names[label])"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Movie Review:\n",
            "\n",
            " b\"This was an absolutely terrible movie. Don't be lured in by Christopher Walken or Michael Ironside. Both are great actors, but this must simply be their worst role in history. Even their great acting could not redeem this movie's ridiculous storyline. This movie is an early nineties US propaganda piece. The most pathetic scenes were those when the Columbian rebels were making their cases for revolutions. Maria Conchita Alonso appeared phony, and her pseudo-love affair with Walken was nothing but a pathetic emotional plug in a movie that was devoid of any real meaning. I am disappointed that there are movies like this, ruining actor's like Christopher Walken's good name. I could barely sit through it.\"\n",
            "\n",
            "Label: negative\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Load Data [Primary Dataset]"
      ],
      "metadata": {
        "id": "F91r5cNzaruD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "drive.mount('/content/drive')\n",
        "import os\n",
        "os.chdir('/content/drive/MyDrive/knowze/sentiment')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F611qf2ma76z",
        "outputId": "a24c65f4-d141-496d-dd41-867c85940229"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load csv file\n",
        "\n",
        "dataset = pd.read_csv('data_sentimenv2.csv')\n",
        "\n",
        "# Cleaning Data\n",
        "dataset['text'] = dataset['text'].str.lower()  # Ubah ke lowercase\n",
        "dataset['text'] = dataset['text'].str.replace('[^\\w\\s]', '')  # Hapus karakter khusus\n",
        "\n",
        "# Turn Text into list\n",
        "sentences = dataset['text'].tolist()\n",
        "labels = dataset['sentiment'].tolist()\n",
        "\n",
        "# for item in range(0,5):\n",
        "#   label = labels[item]\n",
        "#   print(sentences[item] + ' Labels : ')\n",
        "#   print(label)\n",
        "\n",
        "# Turn Text Into tensor\n",
        "sentences_tensor = tf.constant(np.array(sentences))\n",
        "print(sentences_tensor[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tt-nyzkQbcKZ",
        "outputId": "f59c73ff-9f57-412f-c786-88a3f53a2a20"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tf.Tensor(b'kursus ini sangat membantu dan informatif materi disajikan dengan baik', shape=(), dtype=string)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-4-3341c6e05c72>:7: FutureWarning: The default value of regex will change from True to False in a future version.\n",
            "  dataset['text'] = dataset['text'].str.replace('[^\\w\\s]', '')  # Hapus karakter khusus\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Persiapan label agar sesuai dengan panjang teks setelah padding\n",
        "padded_labels = []\n",
        "for label in labels:\n",
        "    # padded_label = label[:max_length]  # Ambil sebagian label sesuai dengan panjang teks yang sudah dipad\n",
        "    padded_labels.append(label)\n",
        "\n",
        "padded_labels = tf.constant(padded_labels)\n",
        "\n",
        "# Tampilkan hasil preprocessing\n",
        "print(padded_labels)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CK0RI7TIc3iF",
        "outputId": "61242aa2-173d-4a74-d7a1-364405a54676"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tf.Tensor(\n",
            "[1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 1 1 0 0 1 0 0 1\n",
            " 1 1 0 0 1 1 0 0 1 1 0 0 1 0 1 1 0 0 1 1 0 1 0 1 0 1 0 1 0 1 1 0 1 1 0 1 0\n",
            " 1 0 1 1 0 0 1 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 1 0 1 0 1 0 1 0 1 1 0\n",
            " 0 1 0 1 1 0 1 0 0 1 0 1 1 0 1 0 0 1 0 1 0 0 1 0 1 0 1 0 0 0 0 1 1 0 0 1 1\n",
            " 0 0 0 1 0 1 0 1 0 0 0 0 1 1 1 0 1 1 0 0 0 1 0 1 1 0 0 1 0 0 0 0 0 1 1 0 0\n",
            " 1 0 0 0 0 0 0 0 1 0 0 0 0 1 0 0 1 1 0 0 1 0 0 0 0 0 0 0 1 0 0 0 0 1 0 0 0\n",
            " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            " 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
            " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
            " 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1\n",
            " 1 1 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0\n",
            " 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            " 0 0 0 0 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 0 0 0 0\n",
            " 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 0 0\n",
            " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1], shape=(590,), dtype=int32)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "max_data_length = tf.size(labels).numpy()\n",
        "max_length= 20\n",
        "max_data_length"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2Pb4yAo9bfGH",
        "outputId": "b290ddca-6c43-4f8e-f03c-ae8fe3619ea3"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "590"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_Tam__wal_2s"
      },
      "source": [
        "## Load Word Embeddings\n",
        "\n",
        "Pake model yang otomatis pre-processing sentences nya"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wbYukNNs2y7H"
      },
      "source": [
        "# if you are running the notebook on Colab\n",
        "# embedding = \"https://tfhub.dev/google/tf2-preview/gnews-swivel-20dim/1\"\n",
        "embedding = \"https://www.kaggle.com/models/google/nnlm/frameworks/TensorFlow2/variations/id-dim50/versions/1\"\n",
        "\n",
        "# # if you are running the notebook on your local machine\n",
        "# embedding = \"./models/tf2-preview_gnews-swivel-20dim_1\"\n",
        "\n",
        "hub_layer = hub.KerasLayer(embedding, input_shape=[], dtype=tf.string ,trainable=False)"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P6V9WM7Gl_2v"
      },
      "source": [
        "## Build Pipeline"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IK1a7HTYl_2w"
      },
      "source": [
        "# batch_size = 512\n",
        "\n",
        "# train_batches = train_data.shuffle(num_train_examples // 4).batch(batch_size).prefetch(1)\n",
        "# validation_batches = validation_data.batch(batch_size).prefetch(1)\n",
        "# test_batches = test_data.batch(batch_size)"
      ],
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Shuffle Data\n",
        "combined_data = tf.data.Dataset.from_tensor_slices((sentences_tensor, padded_labels))\n",
        "# print(list(combined_data.as_numpy_iterator())[0])\n",
        "shuffled_data = combined_data.shuffle(buffer_size=len(sentences_tensor))\n",
        "\n",
        "# Define training and validation sets\n",
        "training_set = shuffled_data.take(500)\n",
        "validation_set = shuffled_data.skip(500)\n",
        "\n",
        "# Configure the batch size and prefetch for better performance\n",
        "batch_size = 32\n",
        "training_set = training_set.batch(batch_size).prefetch(buffer_size=tf.data.AUTOTUNE)\n",
        "validation_set = validation_set.batch(batch_size).prefetch(buffer_size=tf.data.AUTOTUNE)"
      ],
      "metadata": {
        "id": "wTO3UpWXgYyu"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Mengonversi dataset TensorFlow menjadi iterator Numpy\n",
        "training_iterator = iter(training_set)\n",
        "# Mendapatkan beberapa contoh dari set pelatihan\n",
        "num_examples = 1  # Ganti dengan jumlah contoh yang ingin Anda lihat\n",
        "for i in range(num_examples):\n",
        "    example_data = next(training_iterator)\n",
        "    print(\"Contoh Data:\", i + 1)\n",
        "    print(\"Input Data:\", example_data[0])  # Cetak input data\n",
        "    print(\"Label Data:\", example_data[1])  # Cetak label data"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Eu18AVWshJsV",
        "outputId": "cbc03018-2c21-4f72-ecc9-7ecde8623d69"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Contoh Data: 1\n",
            "Input Data: tf.Tensor(\n",
            "[b'belajar memainkan alat musik yang sulit diatur'\n",
            " b'materinya oke lah tapi kurang greget'\n",
            " b'belajar menulis cerita dengan plot yang tak jelas'\n",
            " b'belajar memasak dengan resep yang teruji'\n",
            " b'belajar menyanyi dengan nada yang tidak pas'\n",
            " b'isinya keren sangat informatif'\n",
            " b'materinya lumayan sih tapi kayak kurang aja gitu'\n",
            " b'materi sangat terlalu pendek kurang menjelaskan dengan baik'\n",
            " b'saya senang menemukan kursus ini materi sangat bermanfaat'\n",
            " b'belajar menyusun presentasi yang menarik'\n",
            " b'gak masuk akal gak memberikan manfaat' b'gak bermanfaat banget aneh'\n",
            " b'kursusnya soso aja sih kurang menarik'\n",
            " b'materi sangat detail sangat memperkaya wawasan' b'belajar membuat bom'\n",
            " b'pengembangnya hebat hasilnya luar biasa'\n",
            " b'kursusnya lumayan sih tapi materinya agak membingungkan'\n",
            " b'kontennya kurang greget sih menurutku kurang update gitu'\n",
            " b'saya sangat puas dengan kedalaman materi yang disampaikan'\n",
            " b'cara mengatur keuangan untuk masa depan'\n",
            " b'rekomendasi tinggi untuk kursus ini sangat informatif'\n",
            " b'sangat keren sangat memberikan nilai tambah'\n",
            " b'gak paham ga ada yang masuk akal'\n",
            " b'kontennya kurang oke menurutku agak biasa aja'\n",
            " b'kursus ini memberikan pemahaman yang cepat dan efektif'\n",
            " b'sangat kecewa dengan kualitas materi kursus ini'\n",
            " b'instruktur tidak kompeten sulit untuk mengikuti penjelasannya'\n",
            " b'materinya kurang membantu mending pake chatgpt'\n",
            " b'cara mengembangkan kebiasaan membaca'\n",
            " b'gak ngaruh sama sekali gagal paham'\n",
            " b'cara memulai bisnis online yang sukses'\n",
            " b'tidak puas dengan kursus ini materi tidak terstruktur dengan baik'], shape=(32,), dtype=string)\n",
            "Label Data: tf.Tensor([0 0 0 1 0 1 0 0 1 1 0 0 0 1 0 1 0 0 1 1 1 1 0 0 1 0 0 0 1 0 1 0], shape=(32,), dtype=int32)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vJ8uZcqOl_2y"
      },
      "source": [
        "## Build the Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2mvUjhw02y9-"
      },
      "source": [
        "model = tf.keras.Sequential([\n",
        "        hub_layer,\n",
        "        tf.keras.layers.Dense(32, activation='relu'),\n",
        "        tf.keras.layers.Dense(16, activation='relu'),\n",
        "        tf.keras.layers.Dense(1, activation='sigmoid')])"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K-kJSvK57S5F"
      },
      "source": [
        "## Train the Model\n",
        "\n",
        "Since this is a binary classification problem and the model outputs a probability (a single-unit layer with a sigmoid activation), we'll use the `binary_crossentropy` loss function."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3S8wWDol2zBP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b65da12f-02dc-4448-ab3d-ce0407604fc9"
      },
      "source": [
        "model.compile(optimizer='adam',\n",
        "              loss='binary_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "history = model.fit(training_set,\n",
        "                    epochs=40,\n",
        "                    validation_data=validation_set)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/40\n",
            "16/16 [==============================] - 1s 16ms/step - loss: 0.6845 - accuracy: 0.5780 - val_loss: 0.6695 - val_accuracy: 0.6222\n",
            "Epoch 2/40\n",
            "16/16 [==============================] - 0s 3ms/step - loss: 0.6621 - accuracy: 0.6520 - val_loss: 0.6405 - val_accuracy: 0.7333\n",
            "Epoch 3/40\n",
            "16/16 [==============================] - 0s 3ms/step - loss: 0.6343 - accuracy: 0.7120 - val_loss: 0.6095 - val_accuracy: 0.7778\n",
            "Epoch 4/40\n",
            "16/16 [==============================] - 0s 3ms/step - loss: 0.5961 - accuracy: 0.7420 - val_loss: 0.5562 - val_accuracy: 0.8111\n",
            "Epoch 5/40\n",
            "16/16 [==============================] - 0s 3ms/step - loss: 0.5620 - accuracy: 0.7760 - val_loss: 0.5148 - val_accuracy: 0.8222\n",
            "Epoch 6/40\n",
            "16/16 [==============================] - 0s 3ms/step - loss: 0.5132 - accuracy: 0.8260 - val_loss: 0.4929 - val_accuracy: 0.8111\n",
            "Epoch 7/40\n",
            "16/16 [==============================] - 0s 3ms/step - loss: 0.4664 - accuracy: 0.8380 - val_loss: 0.4709 - val_accuracy: 0.8000\n",
            "Epoch 8/40\n",
            "16/16 [==============================] - 0s 3ms/step - loss: 0.4139 - accuracy: 0.8540 - val_loss: 0.4340 - val_accuracy: 0.8556\n",
            "Epoch 9/40\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 0.3838 - accuracy: 0.8680 - val_loss: 0.3632 - val_accuracy: 0.8778\n",
            "Epoch 10/40\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 0.3459 - accuracy: 0.8900 - val_loss: 0.3173 - val_accuracy: 0.8889\n",
            "Epoch 11/40\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 0.3131 - accuracy: 0.9100 - val_loss: 0.2780 - val_accuracy: 0.9667\n",
            "Epoch 12/40\n",
            "16/16 [==============================] - 0s 3ms/step - loss: 0.2713 - accuracy: 0.9260 - val_loss: 0.3079 - val_accuracy: 0.9000\n",
            "Epoch 13/40\n",
            "16/16 [==============================] - 0s 3ms/step - loss: 0.2586 - accuracy: 0.9140 - val_loss: 0.2499 - val_accuracy: 0.9111\n",
            "Epoch 14/40\n",
            "16/16 [==============================] - 0s 3ms/step - loss: 0.2349 - accuracy: 0.9240 - val_loss: 0.2175 - val_accuracy: 0.9556\n",
            "Epoch 15/40\n",
            "16/16 [==============================] - 0s 3ms/step - loss: 0.2203 - accuracy: 0.9340 - val_loss: 0.2125 - val_accuracy: 0.9444\n",
            "Epoch 16/40\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 0.2081 - accuracy: 0.9400 - val_loss: 0.1472 - val_accuracy: 0.9667\n",
            "Epoch 17/40\n",
            "16/16 [==============================] - 0s 3ms/step - loss: 0.2014 - accuracy: 0.9320 - val_loss: 0.1734 - val_accuracy: 0.9444\n",
            "Epoch 18/40\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 0.1818 - accuracy: 0.9400 - val_loss: 0.1988 - val_accuracy: 0.9333\n",
            "Epoch 19/40\n",
            "16/16 [==============================] - 0s 3ms/step - loss: 0.1681 - accuracy: 0.9480 - val_loss: 0.1660 - val_accuracy: 0.9444\n",
            "Epoch 20/40\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.1696 - accuracy: 0.9480 - val_loss: 0.2051 - val_accuracy: 0.8889\n",
            "Epoch 21/40\n",
            "16/16 [==============================] - 0s 3ms/step - loss: 0.1577 - accuracy: 0.9480 - val_loss: 0.1386 - val_accuracy: 0.9556\n",
            "Epoch 22/40\n",
            "16/16 [==============================] - 0s 3ms/step - loss: 0.1547 - accuracy: 0.9480 - val_loss: 0.1575 - val_accuracy: 0.9556\n",
            "Epoch 23/40\n",
            "16/16 [==============================] - 0s 3ms/step - loss: 0.1363 - accuracy: 0.9620 - val_loss: 0.1196 - val_accuracy: 0.9778\n",
            "Epoch 24/40\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 0.1350 - accuracy: 0.9640 - val_loss: 0.1006 - val_accuracy: 0.9667\n",
            "Epoch 25/40\n",
            "16/16 [==============================] - 0s 3ms/step - loss: 0.1251 - accuracy: 0.9600 - val_loss: 0.0926 - val_accuracy: 0.9778\n",
            "Epoch 26/40\n",
            "16/16 [==============================] - 0s 3ms/step - loss: 0.1224 - accuracy: 0.9720 - val_loss: 0.0857 - val_accuracy: 0.9778\n",
            "Epoch 27/40\n",
            "16/16 [==============================] - 0s 3ms/step - loss: 0.1142 - accuracy: 0.9620 - val_loss: 0.0697 - val_accuracy: 0.9889\n",
            "Epoch 28/40\n",
            "16/16 [==============================] - 0s 3ms/step - loss: 0.1216 - accuracy: 0.9740 - val_loss: 0.1260 - val_accuracy: 0.9444\n",
            "Epoch 29/40\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.1067 - accuracy: 0.9700 - val_loss: 0.1145 - val_accuracy: 0.9667\n",
            "Epoch 30/40\n",
            "16/16 [==============================] - 0s 3ms/step - loss: 0.1130 - accuracy: 0.9720 - val_loss: 0.1042 - val_accuracy: 0.9778\n",
            "Epoch 31/40\n",
            "16/16 [==============================] - 0s 3ms/step - loss: 0.1088 - accuracy: 0.9680 - val_loss: 0.0823 - val_accuracy: 1.0000\n",
            "Epoch 32/40\n",
            "16/16 [==============================] - 0s 3ms/step - loss: 0.1027 - accuracy: 0.9660 - val_loss: 0.1378 - val_accuracy: 0.9667\n",
            "Epoch 33/40\n",
            "16/16 [==============================] - 0s 3ms/step - loss: 0.0988 - accuracy: 0.9780 - val_loss: 0.1067 - val_accuracy: 0.9778\n",
            "Epoch 34/40\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 0.0965 - accuracy: 0.9780 - val_loss: 0.1240 - val_accuracy: 0.9556\n",
            "Epoch 35/40\n",
            "16/16 [==============================] - 0s 3ms/step - loss: 0.0862 - accuracy: 0.9860 - val_loss: 0.0838 - val_accuracy: 0.9778\n",
            "Epoch 36/40\n",
            "16/16 [==============================] - 0s 3ms/step - loss: 0.0793 - accuracy: 0.9840 - val_loss: 0.1110 - val_accuracy: 0.9556\n",
            "Epoch 37/40\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 0.0896 - accuracy: 0.9800 - val_loss: 0.1056 - val_accuracy: 0.9778\n",
            "Epoch 38/40\n",
            "16/16 [==============================] - 0s 3ms/step - loss: 0.0821 - accuracy: 0.9820 - val_loss: 0.0928 - val_accuracy: 0.9667\n",
            "Epoch 39/40\n",
            "16/16 [==============================] - 0s 3ms/step - loss: 0.0862 - accuracy: 0.9780 - val_loss: 0.1104 - val_accuracy: 0.9778\n",
            "Epoch 40/40\n",
            "16/16 [==============================] - 0s 3ms/step - loss: 0.0837 - accuracy: 0.9800 - val_loss: 0.0783 - val_accuracy: 0.9889\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MyORnj-u8F_j"
      },
      "source": [
        "## Evaluate the Model\n",
        "\n",
        "We will now see how well our model performs on the testing set."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ch6hq1_kl_23",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9ad689e5-7c05-4e7e-eefa-9daece03dd5e"
      },
      "source": [
        "eval_results = model.evaluate(validation_set, verbose=0)\n",
        "\n",
        "for metric, value in zip(model.metrics_names, eval_results):\n",
        "    print(metric + ': {:.3}'.format(value))"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loss: 0.146\n",
            "accuracy: 0.967\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "my_array = [[\"Cara membakar rumah orang yang kita benci\"], [\"Cara mulai gym\"], [\"Cara membunuh orang dan menyembunyikannya\"]]\n",
        "print(my_array)\n",
        "\n",
        "prediction = []\n",
        "def predict(list_sentences):\n",
        "  for sentence in list_sentences:\n",
        "    predict  = model.predict(sentence)\n",
        "    prediction.append(predict)\n",
        "  for i in prediction:\n",
        "    print(i)\n",
        "    if i[0] > 0.5:\n",
        "      print('Positif')\n",
        "    else:\n",
        "      print('Negatif')\n",
        "\n",
        "predict(my_array)\n",
        "# print(prediction)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ydmylWPIHOX8",
        "outputId": "e67bb39d-7212-459f-ec9f-c53f3ee85c25"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[['Cara membakar rumah orang yang kita benci'], ['Cara mulai gym'], ['Cara membunuh orang dan menyembunyikannya']]\n",
            "1/1 [==============================] - 0s 45ms/step\n",
            "1/1 [==============================] - 0s 45ms/step\n",
            "1/1 [==============================] - 0s 41ms/step\n",
            "[[0.00068896]]\n",
            "Negatif\n",
            "[[0.96464574]]\n",
            "Positif\n",
            "[[0.48389444]]\n",
            "Negatif\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "# Teks baru yang ingin Anda prediksi\n",
        "new_text = [\"Fuck your movie is so bad\"]\n",
        "\n",
        "# Inisialisasi CountVectorizer untuk tokenisasi\n",
        "vectorizer = CountVectorizer()\n",
        "\n",
        "# Lakukan tokenisasi pada teks\n",
        "vectorizer.fit(new_text)\n",
        "\n",
        "# Ubah teks menjadi representasi vektor\n",
        "text_vector = vectorizer.transform(new_text)\n",
        "\n",
        "# Output hasil tokenisasi\n",
        "print(\"Vocabulary:\")\n",
        "print(vectorizer.vocabulary_)  # Tampilkan daftar kata-kata yang dihasilkan sebagai token\n",
        "\n",
        "print(\"\\nVectorized Text:\")\n",
        "print(text_vector.toarray())  # Tampilkan representasi vektor dari teks berdasarkan token\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "molnB4sDIJV3",
        "outputId": "e5b0516c-1e3b-4511-f47d-ef95e7c1ffdc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Vocabulary:\n",
            "{'fuck': 1, 'your': 5, 'movie': 3, 'is': 2, 'so': 4, 'bad': 0}\n",
            "\n",
            "Vectorized Text:\n",
            "[[1 1 1 1 1 1]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "lkfij_nAJhef"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}